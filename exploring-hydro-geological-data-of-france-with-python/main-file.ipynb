{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "Copie de exploring-hydro-geological-data-of-france-with-python-26-11-2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guiattard/PythonForGeosciences/blob/master/exploring-hydro-geological-data-of-france-with-python/main-file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy8klkZ3UO84"
      },
      "source": [
        "# Exploring (hydro-)geological data of France with python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFr0_Qqy5T6b"
      },
      "source": [
        "#@title Copyright 2020 Guillaume Attard { display-mode: \"form\" }\r\n",
        "#\r\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "# you may not use this file except in compliance with the License.\r\n",
        "# You may obtain a copy of the License at\r\n",
        "#\r\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\r\n",
        "#\r\n",
        "# Unless required by applicable law or agreed to in writing, software\r\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "# See the License for the specific language governing permissions and\r\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihH2Wa7XUO84"
      },
      "source": [
        "by [Guilllaume Attard](https://guillaumeattard.com/)\n",
        "\n",
        "http://pythonforgeosciences.com\n",
        "\n",
        "last update 17-12-2020\n",
        "\n",
        "Notebook status : *Under Review*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHtwRGx8UO84"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In France, main geological/geoscientific data are managed by the [French Geological Survey](https://www.brgm.fr/) and several plaforms provide a public and free access to many datasets such as:\n",
        "- [geological maps](https://infoterre.brgm.fr/page/telechargement-cartes-geologiques),\n",
        "- boreholes, piezometers and wells locations, geological reports and cross sections ([InfoTerre](https://infoterre.brgm.fr/viewer/MainTileForward.do)). In France, this database is known as the BSS (*Base de donnée du Sous-Sol*) which includes more than 700 000 boreholes descriptions,\n",
        "- piezometric time series and physico-chemical analysis from the national monitoring network ([ADES](https://ades.eaufrance.fr/)).\n",
        "\n",
        "These data dedicated to the (hydro-)geological knowledge are used by engineers and scientists to prevent natural risks and disasters (e.g. landslides or settlements), protect groundwater ressources and secure water supply for example. Particularly, these data can be used in many fields such as hydrogeology, geothermal energy and geotechnical applications.\n",
        "\n",
        "These datasets can be explored and download manually by visiting the main platform of the french geological survey [InfoTerre](https://infoterre.brgm.fr/viewer/MainTileForward.do), but when it comes to export a large amout of data, at a regional scale, this task becomes tedious and time consuming. To face this problem, and improve the dissemination of geoscientific data to the public and to scientists, the french geological survey recently developped many geo-webservices in accordance with european open-data requirements. These geo-websercices can help to automate geological data manipulation, analysis and mapping.\n",
        "\n",
        "Consequently, the aim of this article is to present some of these services. Particularly, we are going to:\n",
        "- download and analyse boreholes informations at the scale of the department of our choice: water supply wells and geothermal installations will be identified.\n",
        "- map all these hydrogeological information on an interactive folium map and add the geological tiles of France.\n",
        "- play with two [Hubeau APIs](https://hubeau.eaufrance.fr/) to describe piezometric and physico-chemical characteristics of some groundwater wells: we will learn how to access and analyse hydaulic heads and groundwater quality fluctuations on the groundwater wells we want.\n",
        "\n",
        "## Run me first\n",
        "\n",
        "### Import usefull libraries\n",
        "\n",
        "Before starting we need to install/import some libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2PZs9nCpW03"
      },
      "source": [
        "!apt install gdal-bin python-gdal python3-gdal\n",
        "!pip install fiona shapely pyproj\n",
        "!apt install python3-rtree \n",
        "!pip install git+git://github.com/geopandas/geopandas.git\n",
        "!pip install descartes \n",
        "!pip install git+https://github.com/python-visualization/folium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF0OKOcIUO84"
      },
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "from pyproj import Transformer\n",
        "import os\n",
        "import datetime\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP08uZ_4UO84"
      },
      "source": [
        "### Administrative units of France\r\n",
        "\r\n",
        "To download informations at the scale of a department, we need some a geographical dataset with administrative units of France. We can use the openstreetmap shapefile of french departements. We download this shapefile and put it in a local directory as follow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz-AS5kNUO84"
      },
      "source": [
        "# The url where we can fide the shapefile\n",
        "url_dep = \"http://osm13.openstreetmap.fr/~cquest/openfla/export/departements-20180101-shp.zip\"\n",
        "\n",
        "# The name of the zip file\n",
        "file_dep = \"departements-20180101-shp.zip\"\n",
        "\n",
        "# Command to donwload the file at the given url\n",
        "r = requests.get(url_dep)\n",
        "\n",
        "# Then we open the file\n",
        "open(file_dep, 'wb').write(r.content)\n",
        "\n",
        "# We extract the content of the .zip file\n",
        "with zipfile.ZipFile(file_dep, 'r') as unzip: unzip.extractall(\"dep.shp\")\n",
        "\n",
        "# we finally read the shapefile and make some cleaning\n",
        "dep = gpd.read_file(\"dep.shp\")\n",
        "\n",
        "# We remove the zipfile\n",
        "os.remove(file_dep)\n",
        "\n",
        "# we print the head of our geodataframe\n",
        "dep.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFnisjhhUO85"
      },
      "source": [
        "All department numbers are indicated in the column *code_insee*. We should be careful about this number because the *Rhône* departement is identified as 69D to make the difference with the metropolitan area of Lyon which is identified as 69M. Here, we do not want to distinguish them so we create a new column associating the same number *69*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqk4EgfuUO85"
      },
      "source": [
        "dep[\"num\"] = dep[\"code_insee\"]\n",
        "dep = dep.set_index(\"code_insee\")\n",
        "dep.at[\"69M\", \"num\"] = \"69\"\n",
        "dep.at[\"69D\", \"num\"] = \"69\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQYRR6ZDUO85"
      },
      "source": [
        "### Selection of some departments of interest\r\n",
        "\r\n",
        "To see the result, we can select some departments of intereset and preview their shape. In the following, we will work with this selection of departments numbers to get some (hydro-)geological information. I decide to work with the departement of the Ain (01), the Rhône (69) and of the Isère (38) but you can select others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LptjwbN_UO85"
      },
      "source": [
        "# We select the numbers (as strings) of our department of interest\n",
        "deps_of_interest = ['69', '01', '38']\n",
        "\n",
        "# We reduce our geodataframe to our departement of interest\n",
        "dep = dep.loc[dep[\"num\"].isin(deps_of_interest)]\n",
        "dep.plot()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ortbvJpaUO85"
      },
      "source": [
        "We also download a dataset of main cities of France (more than 100'000 inhabitants) and we keep the cities inside our depatment of interest following a similar procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMYoVODsUO85"
      },
      "source": [
        "# The url where we can fide the shapefile cities\n",
        "url_cit = \"https://simplemaps.com/static/data/country-cities/fr/fr.csv\"\n",
        "\n",
        "# Same procedure as seen before for departments\n",
        "file_cit = \"fr.csv\"\n",
        "\n",
        "r = requests.get(url_cit)\n",
        "open(file_cit, 'wb').write(r.content)\n",
        "\n",
        "cit = pd.read_csv('fr.csv', encoding='utf-8')\n",
        "\n",
        "#We convert it into a geodataframe\n",
        "geo_cit = gpd.GeoDataFrame(cit, geometry = gpd.points_from_xy(cit.lng, cit.lat),\n",
        "                           crs = 'epsg:4326')\n",
        "\n",
        "#We only keep cities inside our dep of interest:\n",
        "geo_cit = gpd.sjoin(geo_cit, dep, op='within')\n",
        "\n",
        "# We sort the associated table by population:\n",
        "geo_cit = geo_cit.sort_values(by=['population_proper'], ascending = False).dropna()\n",
        "\n",
        "# we print the head of our geodataframe\n",
        "geo_cit.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQkM99T5UO85"
      },
      "source": [
        "## Extraction and analysis of boreholes information (BSS data)\r\n",
        "\r\n",
        "### Get your data into a clean (geo-)dataframe\r\n",
        "\r\n",
        "Now, we want to get some information about the geological units of these departments. In the following, we are going to define a function that take the department number as input and which give a dataframe of boreholes information as output. Then, we will be able to identify we groundwater wells or geothermal installations are located and determine the groundwater table depth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5PPfMSUUO86"
      },
      "source": [
        "def download_bss_dep(num):\n",
        "    # We use the URL where we can find all BSS zipfile sorted by department\n",
        "    url_root = \"http://infoterre.brgm.fr/telechargements/ExportsPublicsBSS/\"\n",
        "    file = \"bss_export_\" + num + \".zip\"\n",
        "    url = url_root + file\n",
        "    \n",
        "    # It will be the final path of our file\n",
        "    final_path = \"bss_export_\" + num +\".csv\"\n",
        "    \n",
        "    if os.path.isfile(final_path):\n",
        "        # If we have already download this BSS dep, no need to download it again\n",
        "        print(file +\" already exists. Remove it if an update is necessary.\")\n",
        "        bss = pd.read_csv(final_path, sep=';', decimal=\",\", low_memory=False)\n",
        "    else:\n",
        "        # We download the data and we reproduce the previous process\n",
        "        print(file +\" does not exist. Download in progress.\")\n",
        "        r = requests.get(url)\n",
        "        open(file, 'wb').write(r.content)\n",
        "        with zipfile.ZipFile(file, 'r') as unzip: unzip.extractall()\n",
        "        print(file +\" is now downloaded.\")\n",
        "\n",
        "        # We remove the zip\n",
        "        os.remove(file)\n",
        "\n",
        "        # We build the dataframe\n",
        "        bss = pd.read_csv(final_path, sep=';', decimal=\",\", low_memory=False)\n",
        "    return bss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1iTTkoTUO86"
      },
      "source": [
        "Now we can make a loop to concat the data of our departments of interest:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnYpAlXaUO86"
      },
      "source": [
        "# We create an empty dataframe\n",
        "bss_dataset = pd.DataFrame()\n",
        "\n",
        "# Our loop where we apply our function\n",
        "for num_dep in deps_of_interest:\n",
        "    bss = download_bss_dep(num_dep)\n",
        "    frames = [bss_dataset,bss]\n",
        "    bss_dataset = pd.concat(frames)\n",
        "    print('Departement', num_dep, 'is done.')\n",
        "    \n",
        "bss_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE5QgYFfUO86"
      },
      "source": [
        "As you can see, there are 71 columns: *'ID_BSS', 'indice', 'designation', 'nom_abrege', 'libelle', 'lien_infoterre', 'google_maps', 'coupe_geologique', 'coupe_autre', 'log_geol_verifie', 'nb_scans', 'nb_scans_coupe', 'lex_organisme', 'lex_type_declaration', 'point_eau', 'lex_sgr', 'lex_num_departement', 'lex_nom_departement', 'lex_num_commune', 'lex_insee_commune', 'lex_nom_commune', 'lieu_dit', 'x_saisie', 'y_saisie', 'lex_projection_saisie', 'lex_unite_saisie', 'x_ref06', 'y_ref06', 'lex_projection_ref06', 'lex_precision_xy', 'lex_qual_position', 'z_sol', 'lex_prec_z_sol', 'lex_mode_obtention_z', 'z_bdalti', 'lex_nature', 'prof_investigation', 'prof_accessible', 'diametre_tubage', 'prof_eau_sol', 'lex_type_prof_eau_sol', 'date_eau_sol', 'z_origine_coupe', 'lex_prec_z_origine_coupe', 'date_coupe', 'date_fin_travaux', 'nombre_observations', 'lex_num_carte_geol', 'lex_nom_carte_geol', 'coupure_huitieme', 'lex_etat_physique', 'lex_etat', 'lex_execution', 'lex_utilisation', 'lex_recherche', 'lex_exploitation', 'lex_reconnaissance', 'lex_fonction', 'deb_fonction', 'lex_usage', 'deb_usage', 'lex_gisement', 'lex_documents', 'reference', 'date_dossier', 'date_saisie', 'date_dern_maj', 'procede_geothermique', 'categorie_geothermie', 'usage_geothermie', 'relation_aquifere'*.\n",
        "\n",
        "In the following, we reduce our focus to few relevent attributs (but please add more if you want to):\n",
        "- *ID_BSS*, *indice* and *designation* which are national codes of boreholes. These codes are requested when making queries to different webservices (we'll see that later),\n",
        "- *point_eau* is a bolean indicating if there is some groundwater or not,\n",
        "- *x_ref06* and *y_ref06* are indicating the location in a geographical projection given by *lex_projection_ref06*,\n",
        "- *z_sol* indicates the ground elevation mesured and *z_bdalti* the ground elevation given by the IGN at the same location (can be useful to get both to identify some errors),\n",
        "- *prof_investigation* indicates the total depth of the borehole,\n",
        "- *prof_eau_sol* indicates the groundwater table depth if any exists,\n",
        "- *lex_usage* refer to the usage of the borehole/water well,\n",
        "- *procede_geothermique*, *categorie_geothermie*, and *usage_geothermie* are indicating if the point is actually a geothermla installation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0qaDm5TUO86"
      },
      "source": [
        "items = ['ID_BSS', 'indice', 'designation', 'x_ref06', 'y_ref06','lex_projection_ref06','z_sol', 'z_bdalti', 'point_eau',\n",
        "       'prof_investigation', 'prof_eau_sol', 'lex_usage', 'procede_geothermique',\n",
        "       'categorie_geothermie', 'usage_geothermie']\n",
        "\n",
        "# We reduce our dataframe to relevant items:\n",
        "bss_dataset = bss_dataset[[col for col in items]]\n",
        "\n",
        "# We need to recreate the \"old\" ID of boreholes to access some data (later)\n",
        "bss_dataset['code_bss'] = bss_dataset['indice'] + '/' + bss_dataset['designation']\n",
        "bss_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EAUihxEUO86"
      },
      "source": [
        "Because the projection is not the same for all departements of France (because of overseas departments), the epsg number of different projections must be known. They are listed in a *epsg* dataframe below and finally, a procedure is made to convert all location in longitudes/latitudes (epsg:4326). **The following step is required in case you want to work on overseas departments. In case you won't, the following procedure still works to get longitudes/latitudes from Lambert 93 coordinates**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcmCHujLUO86"
      },
      "source": [
        "# We build a epsg df to associate projection names and epsg codes:\n",
        "epsg_df = pd.DataFrame(columns = ['name', 'code_epsg'])\n",
        "epsg_df = epsg_df.set_index('name')\n",
        "epsg_df.at['Lambert-93', 'code_epsg'] = '2154'\n",
        "epsg_df.at['Antilles-84-RRAF91', 'code_epsg'] = '32620'\n",
        "epsg_df.at['Guyane-95', 'code_epsg'] = '2972'\n",
        "epsg_df.at['Réunion-92', 'code_epsg'] = '2975'\n",
        "epsg_df.at['Mayotte-2004', 'code_epsg'] = '4471'\n",
        "\n",
        "# We create a longitude and latitude column to our dataset:\n",
        "bss_dataset['long'] = \"\"\n",
        "bss_dataset['lat'] = \"\"\n",
        "\n",
        "# We make a loop through different projections to convert all in long/lat\n",
        "for name in epsg_df.index:\n",
        "    if name in set(bss_dataset['lex_projection_ref06'].dropna()):\n",
        "        code = epsg_df.at[name, 'code_epsg']\n",
        "        \n",
        "        # This is the input projection name\n",
        "        inProj = 'epsg:' + code\n",
        "        \n",
        "        # This is the output projection name\n",
        "        outProj = 'epsg:4326'\n",
        "\n",
        "        # We get the X and Y data\n",
        "        x = np.asanyarray(bss_dataset.loc[bss_dataset['lex_projection_ref06'] == name]['x_ref06'])\n",
        "        y = np.asanyarray(bss_dataset.loc[bss_dataset['lex_projection_ref06'] == name]['y_ref06'])\n",
        "        \n",
        "        # We define the transformer\n",
        "        transformer = Transformer.from_crs(inProj, outProj)\n",
        "        \n",
        "        # We apply the transformer\n",
        "        latitude ,longitude = transformer.transform(x, y)\n",
        "        \n",
        "        # We add the long/lat info to the dataframe\n",
        "        bss_dataset.loc[bss_dataset['lex_projection_ref06'] == name, 'long'] = longitude\n",
        "        bss_dataset.loc[bss_dataset['lex_projection_ref06'] == name, 'lat'] = latitude\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "# we remove lines where longitudes/latitudes are empty :\n",
        "bss_dataset = bss_dataset.loc[(bss_dataset['long'] != '') &\n",
        "                              (bss_dataset['lat'] != '')]\n",
        "\n",
        "#Finally we can convert our dataframe into a geodataframe\n",
        "geo_bss = gpd.GeoDataFrame(bss_dataset, \n",
        "                           geometry=gpd.points_from_xy(bss_dataset.long, bss_dataset.lat),\n",
        "                           crs = 'epsg:4326')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPEL8cCxUO86"
      },
      "source": [
        "In practice, there are some errors in the reporting of points coordinates so to remove these errors, we make a spatial join of our geodataframe with the spatial extent of our department of interest: we only keep points inside our departments. Finally we plot the location of boreholes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB8Buh62UO86"
      },
      "source": [
        "# We make some cleaning by removing points which are outside our departments of interest\n",
        "geo_bss = gpd.sjoin(geo_bss, dep, op='within')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "\n",
        "# We plot our deps of interest with the Pastel 1 colormap\n",
        "dep.plot(ax = ax, column = \"num\", cmap = \"Pastel1\", alpha = 0.75)\n",
        "\n",
        "# We add the location of borehole in black\n",
        "geo_bss.plot(ax =ax, color = \"black\", markersize=1, label = \"Borehole\", alpha = 0.25)\n",
        "\n",
        "# We add 5 main cities\n",
        "main_cities = geo_cit.head(5)\n",
        "main_cities.plot(ax =ax, color = \"orange\", marker = 'o', markersize=100, label = 'City')\n",
        "\n",
        "# We add some cities to our map\n",
        "for idx, row in main_cities.iterrows():\n",
        "    txt = plt.annotate(s=row['city'], \n",
        "                 xy=[row['geometry'].x +0.025, row['geometry'].y +0.025],\n",
        "                 horizontalalignment='left',\n",
        "                 color = 'black',\n",
        "                 backgroundcolor=\"w\",\n",
        "                 fontsize = 12)\n",
        "    \n",
        "    txt.set_bbox(dict(facecolor='w', alpha=0.5, edgecolor='w'))\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBjqdHJzUO86"
      },
      "source": [
        "We can of course save our geodataframe as a shapefile, in a RESULTS folder as follow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXnxqShnUO86"
      },
      "source": [
        "dir_res = './RESULTS'\n",
        "try:\n",
        "# Create target Directory\n",
        "    os.mkdir(dir_res)\n",
        "except FileExistsError:\n",
        "    pass\n",
        "print(\"The folder is now created\")\n",
        "\n",
        "geo_bss.to_file(\"./RESULTS/all_boreholes.shp\")\n",
        "print(\"The file is saved\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMs3GcdLUO86"
      },
      "source": [
        "### Identifying groundwater wells and geothermal installations\n",
        "\n",
        "Of course we can decide to distinguish the kind of borehole we want to keep. For example, let's select (1) boreholes where a groundwater table depth is reported and (2) geothermal installations.\n",
        "\n",
        "In the first case, we keep points where the attribute *point_eau* is *OUI* (meaning that there is some groundwater right here) AND where *prof_eau_sol* is not a NaN value (meaning that we should know the groundwater table depth).\n",
        "\n",
        "In the second case, we keep points where *categorie_geothermie* OR *procede_geothermique* OR *usage_geothermie* is not a NaN value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUW3rbKyUO86"
      },
      "source": [
        "# We create a new geodataframe with points where we know a groundwater table depth\n",
        "geo_ww = geo_bss.loc[(geo_bss['point_eau'] == 'OUI') & (geo_bss['prof_eau_sol'].notna())]\n",
        "\n",
        "# We only keep relevent features\n",
        "geo_ww = geo_ww[['ID_BSS', 'code_bss', 'z_sol', 'z_bdalti','prof_investigation',\n",
        "                 'prof_eau_sol', 'geometry']]\n",
        "                 \n",
        "# We create a new geodataframe with points where a geotehrmal activity is known\n",
        "geoth_points = geo_bss.loc[geo_bss['categorie_geothermie'].notna() |\n",
        "                    geo_bss['procede_geothermique'].notna() |\n",
        "                    geo_bss['usage_geothermie'].notna()]\n",
        "\n",
        "# We plot the result\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "\n",
        "# We plot our deps of interest with the Pastel 1 colormap\n",
        "dep.plot(ax = ax, column = \"num\", cmap = \"Pastel1\", alpha = 0.75)\n",
        "\n",
        "# We add the location of borehole with water level in blue\n",
        "geo_ww.plot(ax =ax, color = \"blue\", markersize=1, label = \"Borehole with a groundwater table level\")\n",
        "\n",
        "# We add the location of geothermal installations in red\n",
        "geoth_points.plot(ax =ax, color = \"red\", markersize=5, label = \"Geothermal installation\")\n",
        "\n",
        "\n",
        "# We add some cities to our map\n",
        "main_cities.plot(ax =ax, color = \"orange\", marker = 'o', markersize=100, label = 'City')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtHDhSCqUO86"
      },
      "source": [
        "### Describing groundwater table elevation\n",
        "\n",
        "Now that we know where groundwater bodies are located, we might want to describe groundwater levels and maybe describe gorundwater flow directions.\n",
        "\n",
        "To do so, we refine a location of interest by choosing the coordinate of a point, we also define a relevent buffer zone around this point to reduce our dataset to this area of interest. In our example, we select the city of Lyon, and a buffer zone of 10km.\n",
        "\n",
        "**Please note that groundwater table elevation fluctuate over time. In the following, the calculated value represents the water table elevation observed when the borehole was drilled. Also, it should be noted that the calculated values are not synchronous which can alter the local representativity regarding the interpretation groundwater flow directions and flow systems.**\n",
        "\n",
        "*Please note that we could instead import the polygon shapefile of our choice and select boreholes inside our polygon.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFLi7K0-qtND"
      },
      "source": [
        "# we define the long./lat. of our location of interest\n",
        "site_lon, site_lat = 4.8400, 45.7600 \n",
        "\n",
        "site = pd.DataFrame({'Name': ['Lyon'],\n",
        "                     'lat': [site_lat],\n",
        "                     'lon': [site_lon]})\n",
        "\n",
        "# We then build a geopandas dataframe to create our buffer zone:\n",
        "buff_zone = gpd.GeoDataFrame(site, geometry = gpd.points_from_xy(site.lon, site.lat), \n",
        "                          crs = \"epsg:4326\")\n",
        "\n",
        "# to apply a buffer zone, we need to change our projeciton system to work in a metric system.\n",
        "# In France, the common metric projection system is \"epsg:2154\"\n",
        "\n",
        "buff_zone = buff_zone.to_crs(\"epsg:2154\")\n",
        "\n",
        "# we create our buffer zone of 10km:\n",
        "buff_zone[\"geometry\"] = buff_zone[\"geometry\"].buffer(10000)\n",
        "\n",
        "# We get back into epsg:4326 once our buffer zone is defined\n",
        "buff4326 = buff_zone.to_crs(\"epsg:4326\")\n",
        "buff4326 = buff4326[['geometry']]\n",
        "\n",
        "# We finally keep water wells informations located inside this studied area\n",
        "geo_ww = gpd.sjoin(geo_ww, buff4326, op='within')\n",
        "\n",
        "# We keep the columns we want:\n",
        "geo_ww = geo_ww[['ID_BSS', 'code_bss', 'z_sol', 'z_bdalti','prof_investigation',\n",
        "                 'prof_eau_sol', 'geometry']]\n",
        "\n",
        "# We plot the result\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "ax.set_xlim((min(geo_ww['geometry'].x), max(geo_ww['geometry'].x)))\n",
        "ax.set_ylim((min(geo_ww['geometry'].y), max(geo_ww['geometry'].y)))\n",
        "\n",
        "# We plot our deps of interest with the Pastel 1 colormap\n",
        "dep.plot(ax = ax, column = \"num\", cmap = \"Pastel1\", alpha = 0.75)\n",
        "\n",
        "# We add the location of borehole with water level in blue\n",
        "geo_ww.plot(ax =ax, color = \"blue\", markersize=1, label = \"Borehole with a groundwater table level\")\n",
        "\n",
        "# We add the location of geothermal installations in red\n",
        "geoth_points.plot(ax =ax, color = \"red\", markersize=5, label = \"Geothermal installation\")\n",
        "\n",
        "# We add our city of interest\n",
        "ax.scatter(site_lon, site_lat, color = \"orange\", marker = 'o', s=100, label = 'Point of interest')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgkK_TlEUO86"
      },
      "source": [
        "Now let's describe data inside our geodataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ3_Fy_jUO86"
      },
      "source": [
        "# First we convert all interesting fields to numeric values\n",
        "for col in ['z_sol', 'z_bdalti', 'prof_investigation', 'prof_eau_sol']:\n",
        "    geo_ww[col] = pd.to_numeric(geo_ww[col])\n",
        "\n",
        "# Then we can describe our geodataframe\n",
        "geo_ww.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xef0IxuUO86"
      },
      "source": [
        "The result shows some errors regarding the ground elevation (-999 value). Consequently, let's remove data outside first and last percentiles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byvHSsAcUO86"
      },
      "source": [
        "# We select rows inside the range we want\n",
        "geo_ww = geo_ww[(geo_ww.z_sol > geo_ww.z_sol.quantile(0.01)) &\n",
        "                (geo_ww.z_sol < geo_ww.z_sol.quantile(0.99))]\n",
        "\n",
        "# Then we can describe our new geodataframe\n",
        "geo_ww.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysb6QNzrUO87"
      },
      "source": [
        "Now, we define the groundwater table elevation as the ground elevation minus the groundwater table depth. Because we have two different possibility to choose the ground elevation from the IGN *z_bdalti*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg0gcauYUO87"
      },
      "source": [
        "geo_ww.loc[:, 'WT_elevation'] = geo_ww['z_bdalti'] - geo_ww['prof_eau_sol'] \n",
        "geo_ww.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e85ThK2qrags"
      },
      "source": [
        "In addition, we can consider that in our area of interest we focus on the shallow aquifer where groundwater is flowing in the modern alluvial deposit. Consequently, let's focus on an investigation depth ranging from 10m to 30m:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ri5w2pqUO87"
      },
      "source": [
        "# We select wells with a relevant depth:\n",
        "geo_ww = geo_ww[(geo_ww.prof_investigation >= 10) &\n",
        "                (geo_ww.prof_investigation <= 30)]\n",
        "\n",
        "geo_ww = geo_ww.dropna()\n",
        "\n",
        "# We can now have a overview of different features of our dataset:\n",
        "geo_ww[[\"prof_investigation\", \"z_bdalti\", \"WT_elevation\"]].hist()\n",
        "#plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KtBu1taUO87"
      },
      "source": [
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "# We plot the result\n",
        "fig, ax = plt.subplots(figsize=(10,8))\n",
        "\n",
        "ax.set_xlim((min(geo_ww['geometry'].x), max(geo_ww['geometry'].x)))\n",
        "ax.set_ylim((min(geo_ww['geometry'].y), max(geo_ww['geometry'].y)))\n",
        "\n",
        "# We plot our deps of interest with the Pastel 1 colormap\n",
        "dep.plot(ax = ax, column = \"num\", cmap = \"Pastel1\", alpha = 0.75)\n",
        "\n",
        "vmin = geo_ww[\"WT_elevation\"].quantile(0.1)\n",
        "vmax = geo_ww[\"WT_elevation\"].quantile(0.9)\n",
        "\n",
        "# We add the location of borehole with water level in blue\n",
        "geo_ww.plot(ax = ax, cmap = \"viridis_r\",\n",
        "            column = \"WT_elevation\",\n",
        "            markersize=10, \n",
        "            vmin = vmin,\n",
        "            vmax = vmax,\n",
        "            legend = True,\n",
        "            legend_kwds={'label': \"elevation [m a.s.l]\"})\n",
        "\n",
        "ax.set_title(\"Groundwater table elevation\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCatMmD8UO87"
      },
      "source": [
        "## Interactive mapping with folium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qts-DWh_UO87"
      },
      "source": [
        "The information above can easily be projected on an interactive map using the *folium* library. Additionnaly, it is interesting to do so because it can help us to understand the relation with geological units.\n",
        "\n",
        "In fact, since a few years, geological tiles are available across france and can be projected and visualised using *folium*. All WMF/WFS services provided by the BRGM are listed in a table at [this page](http://geoservices.brgm.fr). It is indicated that the geological map can be visualised using the following url *http://geoservices.brgm.fr/geologie*, and the following layer *GEOLOGIE*. We directly use these input to define our WMS tile layer (see below in the script)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYJGNoE7UO87"
      },
      "source": [
        "Then we can build our map using *folium*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWkStAgvUO87"
      },
      "source": [
        "# We import the folium library\n",
        "import folium\n",
        "\n",
        "# We need to import branca.colormap to give pretty colors to our points according to groundwater\n",
        "# table elevation\n",
        "import branca.colormap as cm\n",
        "\n",
        "# We build our map focusing on our site and specifying the zoom start\n",
        "mymap = folium.Map(location=[site_lat, site_lon], \n",
        "                   zoom_start=12,\n",
        "                   control_scale = True)\n",
        "\n",
        "# Add the geological map of France\n",
        "url = 'http://geoservices.brgm.fr/geologie'\n",
        "layer1 = 'GEOLOGIE'\n",
        "folium.WmsTileLayer(url, layer1, attr = 'BRGM', name = 'toto').add_to(mymap)\n",
        "#folium.TileLayer('http://geoservices.brgm.fr/geologie/GEOLOGIE', attr = 'BRGM', name = 'toto').add_to(mymap)\n",
        "\n",
        "# Add a feature group to add borehole with groundwater table elevation\n",
        "fg_gwt = folium.FeatureGroup(name = 'Groundwater table elevation')\n",
        "mymap.add_child(fg_gwt)\n",
        "\n",
        "colormap = cm.LinearColormap(colors=['orange', 'yellow', 'green', 'lightblue', 'blue'], \n",
        "                             index=[geo_ww[\"WT_elevation\"].quantile(0.1),\n",
        "                                    geo_ww[\"WT_elevation\"].quantile(0.25),\n",
        "                                    geo_ww[\"WT_elevation\"].quantile(0.5),\n",
        "                                    geo_ww[\"WT_elevation\"].quantile(0.75),\n",
        "                                    geo_ww[\"WT_elevation\"].quantile(0.9)], \n",
        "                             vmin=geo_ww[\"WT_elevation\"].quantile(0.1),\n",
        "                             vmax=geo_ww[\"WT_elevation\"].quantile(0.9))\n",
        "\n",
        "# We add the caption of our colormap\n",
        "colormap.caption = 'Groundwater table elevation [m asl]'\n",
        "\n",
        "# We give an explicit description of longitude and latitude of our points:\n",
        "geo_ww['lon'] = geo_ww['geometry'].x\n",
        "geo_ww['lat'] = geo_ww['geometry'].y\n",
        "\n",
        "#before interating over our dataset we reset its index\n",
        "#geo_ww = geo_ww.reset_index()\n",
        "# We add all points of our dataset\n",
        "for i, h in zip(geo_ww.index.values, geo_ww.WT_elevation.values):\n",
        "    dfi = folium.CircleMarker(\n",
        "        location = [geo_ww.at[i,'lat'], geo_ww.at[i,'lon']],\n",
        "        popup =str(round(h,2)) + \" m asl\",\n",
        "        radius= 7,\n",
        "        fill=True,\n",
        "        fill_opacity=0.7,\n",
        "        color = colormap(h),\n",
        "        fill_color = colormap(h))\n",
        "    dfi.add_to(fg_gwt)\n",
        "\n",
        "mymap.add_child(colormap)\n",
        "mymap.add_child(folium.map.LayerControl(collapsed=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFhG0J4OUO87"
      },
      "source": [
        "Of couse we can save this map in *html* and open it later with any navigator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGBddpX2UO87"
      },
      "source": [
        "mymap.save(\"my_Python_for_Geosciences_map.html\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf_7efq9UO87"
      },
      "source": [
        "## Access to hydraulic head monitoring data\n",
        "\n",
        "Since a couple of years, several APIs dedicated to water management have been developped on a national platfrom called [Hubeau](https://hubeau.eaufrance.fr/page/apis). Some of these APIs are dedicated to hydrobiology and surface water monitoring, and others to quantitative and qualitative description of groundwater.\n",
        "\n",
        "The principle is quite easy: you provide the API of your choice with (1) one or several borehole identifiers, (2) a period of interest, (3) some parameters of interest (piezometric level, temperature, etc.) and you get back a dictionnary with all information.\n",
        "\n",
        "First the procedure to describe the hydraulic head fluctuation at a given borehole will be described. Secondly, we will focus on how to find all recording stations around a area of interest, for a given period of time.\n",
        "\n",
        "### Getting hydraulic head fluctuations\n",
        "\n",
        "Let's see how the groundwater piezometric API works. We first need define one borehole identifier *code_bss*, and a period of interest:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeYGaZAqUO87"
      },
      "source": [
        "# Definition of an identifier:\n",
        "mycode = \"06987A0186/S\"\n",
        "\n",
        "# Definition of a period of interest:\n",
        "mydate_i = \"2015-01-01\" #year/month/day\n",
        "mydate_f = \"2020-12-31\" #year/month/day"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Aioa3MUO87"
      },
      "source": [
        "To query the API, we need to build an URL which depends on previous parameters. I give the function below providing you the appropriate URL:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7pftcbZUO87"
      },
      "source": [
        "def getHubeauURL_piezo(code_bss, date_i, date_f):\n",
        "    url_root = \"https://hubeau.eaufrance.fr/api/v1/niveaux_nappes/chroniques?\"\n",
        "    url_code = \"code_bss=\" + code_bss.replace(\"/\",\"%2F\")\n",
        "    url_date = \"&date_debut_mesure=\" + date_i + \"&date_fin_mesure=\" + date_f\n",
        "    url_tail = \"&size=20000&sort=asc\"\n",
        "    return url_root + url_code + url_date + url_tail"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9sIwluPUO87"
      },
      "source": [
        "We can now call this function as follow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbhQ-sORUO87"
      },
      "source": [
        "myurl = getHubeauURL_piezo(mycode, mydate_i, mydate_f)\n",
        "\n",
        "print(myurl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lCFce3XUO87"
      },
      "source": [
        "You can now make our query using the *resquests.get* function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1koaPrWfUO87"
      },
      "source": [
        "# We get the content of the webpage\n",
        "r = requests.get(myurl)\n",
        "\n",
        "# The server response:\n",
        "print(r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgWvOxXmUO87"
      },
      "source": [
        "The code '200' means that our query is successful (the code is valid and there are some data available). Let's organize the result (using the *json* method) and print the content of the webpage using the following procedure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euac5skIUO87"
      },
      "source": [
        "# We organize the content into a json dictionnary\n",
        "res = r.json()\n",
        "\n",
        "# We print the result\n",
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OxPgLliUO88"
      },
      "source": [
        "The resulting content is a dictionnary organided according to the date of measurements. We now need to define a function to extract the information we want in this dictionnary. The first key of this dictionnary gives us a *count* of all measurements over our period of interest. All piezometric values are stored in a *data* sub-dictionnary. Let's see on the first element of the *data* dictionnary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8WRHuNuUO88"
      },
      "source": [
        "res['data'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRYK0rYgUO88"
      },
      "source": [
        "We have here a new dictionnary with the ID of our borehole, the date of the measurement (*date_mesure*), the associated timestamp (*timestamp_mesure*), the hydraulic head measured (*niveau_nappe_eau*) and other informations.\n",
        "\n",
        "We can make a function to convert this dictionnary into a dataframe keeping only relevant features as follow: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smj2vRMZUO88"
      },
      "source": [
        "def piezoDico_to_pandas_df(res):\n",
        "    # We convert the data into a dataframe\n",
        "    df = pd.DataFrame.from_dict(res[\"data\"])\n",
        "\n",
        "    # We only keep relevant columns\n",
        "    df = df[['date_mesure', 'niveau_nappe_eau']]\n",
        "\n",
        "    # We rename columns\n",
        "    df = df.rename(columns = {'date_mesure':'date',\n",
        "                    'niveau_nappe_eau': 'hydraulic_head'})\n",
        "\n",
        "    # We make sure that the hydraulic head is numeric\n",
        "    df['hydraulic_head'] = pd.to_numeric(df['hydraulic_head'])\n",
        "\n",
        "    # We convert the date as a datetime format and we define it as index\n",
        "    df['date'] = pd.to_datetime(df['date'], format =  \"%Y-%m-%d\")\n",
        "    df = df.set_index(\"date\").to_period('d')\n",
        "    return df\n",
        "\n",
        "df = piezoDico_to_pandas_df(res)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da4DwYIKUO88"
      },
      "source": [
        "We can finally plot the hydraulic head fluctuation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwELVob0UO88"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "ax.set_title('Hydraulic head fluctuation at ' + mycode)\n",
        "\n",
        "# We plot the mean hydraulic head over the period\n",
        "df['hydraulic_head'].plot(ax = ax, \n",
        "              marker='o', \n",
        "              alpha=0.5,\n",
        "              linestyle='--')\n",
        "\n",
        "ax.set_ylabel('Hydraulic head (m)')\n",
        "ax.set_xlabel('')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQngLTEsUO88"
      },
      "source": [
        "### Finding piezometric monitoring stations around a location of interest\r\n",
        "\r\n",
        "A procedure exists to find all stations with hydraulic head records around a location of interest. What you need is to define the longitude and latitude coordinates of a rectangle area. Let's take the smallest rectangle including our previous circular buffer zone:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_hk-l7iUO88"
      },
      "source": [
        "# We define our rectangle of interest:\n",
        "rect = gpd.GeoDataFrame(geometry = buff4326.envelope)\n",
        "\n",
        "# We take put the coordinates into a list:\n",
        "pnts = np.dstack(rect.at[0, 'geometry'].boundary.xy)\n",
        "pnts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZltB-tYUO88"
      },
      "source": [
        "To make our query, we only need the lower left and top right corners coordinates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN8gI_eiUO88"
      },
      "source": [
        "llc = pnts[0][0]\n",
        "trc = pnts[0][2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVhd4MJiUO88"
      },
      "source": [
        "We then need to make a new function to produce the URL and an other one to organize the result in a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UbmgfSjUO88"
      },
      "source": [
        "def getHubeauURLstations(param, date_research, bbox):\n",
        "    \"\"\"\n",
        "    The aim of this function is to return the URL giving us available stations\n",
        "    depending on:\n",
        "    - param (str): the type of data we are looking for: param = \"piezo\" or \"quality\"\n",
        "    - date_research (str): the date around when we want data ex. = \"2020-01-01\"\n",
        "    - bbox: the area of research (list of two points coordinates). The fist element of the fist\n",
        "    regroups coordinates of the lower left corner and the second element the top right corner.\n",
        "    the function returns a URL\n",
        "    \"\"\"\n",
        "    #param is \"piezo\" or \"quality\"\n",
        "    if param == \"piezo\":\n",
        "        url_root = \"https://hubeau.eaufrance.fr/api/v1/niveaux_nappes/stations?\"\n",
        "    elif param == \"quality\":\n",
        "        url_root = \"https://hubeau.eaufrance.fr/api/v1/qualite_nappes/stations?\"\n",
        "    else:\n",
        "        return print(\"error with param\")\n",
        "\n",
        "    llc = bbox[0]\n",
        "    trc = bbox[1]\n",
        "        \n",
        "    llc_x, llc_y = str(round(llc[0],4)), str(round(llc[1],4))\n",
        "    trc_x, trc_y = str(round(trc[0],4)), str(round(trc[1],4))\n",
        "    \n",
        "    url_bbox = \"bbox=\" + llc_x + \"%2C\" + llc_y + \"%2C\" + trc_x + \"%2C\" + trc_y\n",
        "    url_date = \"&date_recherche=\" + date_research\n",
        "    url_tail = \"&format=json&size=20000\"\n",
        "    return url_root + url_bbox + url_date + url_tail\n",
        "\n",
        "def StationsToDF(url):\n",
        "    \"\"\"\n",
        "    The aim of this function is to organize the data of the resquested stations\n",
        "    into a pandas df.\n",
        "    The function takes an URL as input (str) and returns a pandas df\n",
        "    \"\"\"\n",
        "    # We get the content of the webpage\n",
        "    r = requests.get(url)\n",
        "    # We organize the content into a json dictionnary\n",
        "    res = r.json()\n",
        "    # We convert the data into a dataframe\n",
        "    df = pd.DataFrame.from_dict(res[\"data\"])\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFrZQThZUO88"
      },
      "source": [
        "Let's run this function to sew how many piezometric stations are working/recording at a given date: the 1st of january 2020."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fK85z9YUO88"
      },
      "source": [
        "new_url = getHubeauURLstations(\"piezo\", \"2020-01-01\", [llc, trc])\n",
        "stations = StationsToDF(new_url)\n",
        "\n",
        "stations = stations.set_index(\"code_bss\")\n",
        "stations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNJ72ykOUO88"
      },
      "source": [
        "We can of course print the location of these active stations on an interactive folium map:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl12L0mqUO88"
      },
      "source": [
        "# We import the folium library\n",
        "import folium\n",
        "\n",
        "# We build our map focusing on our site and specifying the zoom start\n",
        "mymap = folium.Map(location=[site_lat, site_lon], \n",
        "                   zoom_start=11,\n",
        "                   show=True, control_scale = True,\n",
        "                   width = 400)\n",
        "\n",
        "# Add the geological map of France\n",
        "url = 'http://geoservices.brgm.fr/geologie'\n",
        "layer1 = 'GEOLOGIE'\n",
        "folium.WmsTileLayer(url, layer1, attr = 'BRGM', name = 'Geological map', show=True).add_to(mymap)\n",
        "\n",
        "# Add a feature group to add borehole with active piezometric stations\n",
        "fg_pz = folium.FeatureGroup(name = 'Active piezometric stations', show = True)\n",
        "mymap.add_child(fg_pz)\n",
        "\n",
        "# We add all points of our dataset\n",
        "for code_bss in stations.index.values:\n",
        "    pzi = folium.Marker(\n",
        "        location = [stations.at[code_bss,'y'], stations.at[code_bss,'x']],\n",
        "        popup = str(code_bss))\n",
        "    pzi.add_to(fg_pz)\n",
        "\n",
        "mymap.add_child(folium.map.LayerControl(collapsed=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__uuq5whUO88"
      },
      "source": [
        "Now, we can plot the piezometric levels of these stations over the similar period of interest that we defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-P4joyyUO88"
      },
      "source": [
        "# Definition of a period of interest:\n",
        "mydate_i = \"2015-01-01\" #year/month/day\n",
        "mydate_f = \"2020-01-01\" #year/month/day\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "for code in stations.index:\n",
        "    myurl = getHubeauURL_piezo(code, mydate_i, mydate_f)\n",
        "    r = requests.get(myurl)\n",
        "    res = r.json()\n",
        "    df = piezoDico_to_pandas_df(res)\n",
        "    \n",
        "\n",
        "    # We plot the mean hydraulic head over the period\n",
        "    df['hydraulic_head'].plot(ax = ax, \n",
        "                  marker='o', \n",
        "                  alpha=0.5,\n",
        "                  linestyle='--',\n",
        "                  label = code)\n",
        "\n",
        "ax.set_ylabel('Hydraulic head (m)')\n",
        "ax.set_xlabel('')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSUPZGhuUO88"
      },
      "source": [
        "## Access to physico-chemical parameters\r\n",
        "\r\n",
        "Similarly, we can make some queries to observe the fluctuation of some physico-chemical parameters such as groundwater temperature, nitrates concentrations, pesticides... many other parameters are availble.\r\n",
        "\r\n",
        "### Exploration of accessible physico-chemical parameters\r\n",
        "\r\n",
        "Since 1992, the French producers of public water data have engaged a consistency process for their data in the framework of the French water information system. Consequently, all groundwater quality parameters are associated to a number in a referetial SANDRE.\r\n",
        "\r\n",
        "This referential can be download below. We can see that 1845 parameters are available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5ngET3uUO89"
      },
      "source": [
        "url_root= \"http://www.sandre.eaufrance.fr/sites/default/files/document-sandre/\"\n",
        "file = \"correspondance_par_sise_sandre_18092018.xls\"\n",
        "url = url_root + file\n",
        "\n",
        "r = requests.get(url)\n",
        "\n",
        "open(file, 'wb').write(r.content)\n",
        "sandre = pd.ExcelFile(file).parse(0)\n",
        "new_header = sandre.iloc[0]\n",
        "sandre = sandre[1:]\n",
        "sandre.columns = new_header\n",
        "sandre = sandre[['Nom_famille_sise', 'nom_parametre_sise', 'code_parametre_sandre']]\n",
        "\n",
        "sandre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-mOQM9LUO89"
      },
      "source": [
        "We could explore all accessible parameters, but in this tutorial we focus on two of them:\n",
        "- temperature with the code 1301,\n",
        "- nitrates concentration with the code 1340.\n",
        "\n",
        "It can be check by printing the appropriate lines of the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um3rtpPAUO89"
      },
      "source": [
        "sandre.loc[(sandre.code_parametre_sandre == 1301) |\n",
        "           (sandre.code_parametre_sandre == 1340)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIjTXWdYUO89"
      },
      "source": [
        "The information about these parameters (units, etc.) are described in the following webpages (in french):\n",
        "- [Nitrates - SANDRE](http://www.sandre.eaufrance.fr/urn.php?urn=urn:sandre:donnees:PAR:FRA:code:1340:::referentiel:2:html)\n",
        "- [Temperature - SANDRE](http://www.sandre.eaufrance.fr/urn.php?urn=urn:sandre:donnees:PAR:FRA:code:1301:::referentiel:2:html)\n",
        "\n",
        "You can easly access and get the information about the parameter you want by using the following function depending on the parameter code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7IkEmn_UO89"
      },
      "source": [
        "def getSandreURL(code_param):\n",
        "    \"\"\"\n",
        "    This function prints the SANDRE information associated to the code (input)\n",
        "    \"\"\"\n",
        "    url_head = \"http://www.sandre.eaufrance.fr/urn.php?urn=urn:sandre:donnees:PAR:FRA:code:\"\n",
        "    code_param = str(code_param)\n",
        "    url_tail = \":::referentiel:2:html\"\n",
        "    url = url_head + code_param + url_tail\n",
        "    print(url)\n",
        "    \n",
        "# Example:\n",
        "getSandreURL(1301)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhlLid5tUO89"
      },
      "source": [
        "These codes must be specified when making queries with the different APIs.\r\n",
        "\r\n",
        "### Finding quality monitoring stations around a location of interest\r\n",
        "\r\n",
        "First, let's find some quality monitoring stations around our location of interest by using the function defined ealier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F39XRs4jUO89"
      },
      "source": [
        "q_url = getHubeauURLstations(\"quality\", \"2020-01-01\", [llc, trc])\n",
        "q_stations = StationsToDF(q_url)\n",
        "q_stations.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Al3T1DWUO89"
      },
      "source": [
        "We can see that there is 471 stations. However, the dates are not necessarily matching our query (it is probably a issue that will be solved in a future version of the API). To refine our selection on our period of interest we convert all date features in datetimes format and compare with our datetimes of interest:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQu_mPj4UO89"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "q_stations['date_debut_mesure'] = pd.to_datetime(q_stations['date_debut_mesure'], format =  \"%Y-%m-%d\")\n",
        "q_stations['date_fin_mesure'] = pd.to_datetime(q_stations['date_fin_mesure'], format =  \"%Y-%m-%d\")\n",
        "\n",
        "datetime_i = datetime.strptime(mydate_i, \"%Y-%m-%d\")\n",
        "datetime_f = datetime.strptime(mydate_f, \"%Y-%m-%d\")\n",
        "\n",
        "q_stations = q_stations.loc[(q_stations['date_debut_mesure'] <= datetime_i) &\n",
        "                            (q_stations['date_fin_mesure'] >= datetime_f)]\n",
        "\n",
        "q_stations = q_stations.set_index(\"code_bss\")\n",
        "q_stations.head()       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIEWsAToUO89"
      },
      "source": [
        "We see that we have finally 7 active stations and we can add them (red color) to our interactive map:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP-n1HrhUO89"
      },
      "source": [
        "# We build our map focusing on our site and specifying the zoom start\n",
        "mymap = folium.Map(location=[site_lat, site_lon], \n",
        "                   zoom_start=11,\n",
        "                   show=True, control_scale = True,\n",
        "                   width = 400)\n",
        "\n",
        "# Add the geological map of France\n",
        "url = 'http://geoservices.brgm.fr/geologie'\n",
        "layer1 = 'GEOLOGIE'\n",
        "folium.WmsTileLayer(url, layer1, attr = 'BRGM', name = 'Geological map', show=True).add_to(mymap)\n",
        "\n",
        "# Add a feature group to add borehole with active quality stations\n",
        "fg_quality = folium.FeatureGroup(name = 'Active groundwater quality stations', show = True)\n",
        "mymap.add_child(fg_quality)\n",
        "\n",
        "# We add all points of our quality dataset\n",
        "for code_bss in q_stations.index.values:\n",
        "    qi = folium.Marker(\n",
        "        location = [q_stations.at[code_bss,'latitude'], q_stations.at[code_bss,'longitude']],\n",
        "        popup = str(code_bss),\n",
        "        icon=folium.Icon(color='red', icon='info-sign'))\n",
        "    qi.add_to(fg_quality)\n",
        "    \n",
        "# Add a feature group to add borehole with active piezometric stations\n",
        "fg_pz = folium.FeatureGroup(name = 'Active piezometric stations', show = True)\n",
        "mymap.add_child(fg_pz)\n",
        "\n",
        "# We add all points of our dataset\n",
        "for code_bss in stations.index.values:\n",
        "    pzi = folium.Marker(\n",
        "        location = [stations.at[code_bss,'y'], stations.at[code_bss,'x']],\n",
        "        popup = str(code_bss))\n",
        "    pzi.add_to(fg_pz)\n",
        "\n",
        "mymap.add_child(folium.map.LayerControl(collapsed=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9VDfnPKUO89"
      },
      "source": [
        "### Getting groundwater temperature and nitrates concentrations in a water well\r\n",
        "\r\n",
        "We first need define one borehole identifier code_bss, a period of interest and the code of parameters we want to explore (temperature and nitrates in our example):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahtIhg8dUO89"
      },
      "source": [
        "# Definition of an identifier:\n",
        "code_ex = \"08036X1858/F2\"\n",
        "\n",
        "# Definition of a period of interest:\n",
        "date_initial_ex = \"2017-01-01\" #year/month/day\n",
        "date_final_ex = \"2019-12-31\" #year/month/day\n",
        "\n",
        "params_ex = [1301, 1340] #our parameters in a list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uo93VqqUO89"
      },
      "source": [
        "To query the API, we need to build an URL which depends on previous parameters. I give the function below providing you the appropriate URL:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AC0jPIlUO89"
      },
      "source": [
        "def getHubeauURLquality(code, date_initial, date_final, params):\n",
        "    \"\"\"\n",
        "    this function return the url needed to get the required parameters\n",
        "    of the borehole code\n",
        "    \"\"\"\n",
        "    url_head = \"https://hubeau.eaufrance.fr/api/v1/qualite_nappes/analyses?\"\n",
        "    url_code = \"bss_id=\" + str(code).replace(\"/\",\"%2F\")\n",
        "    url_params = \"&code_param=\" \n",
        "    for par in params:\n",
        "        url_params += str(par) + \"%2C%20\"\n",
        "        \n",
        "    url_params = url_params[:-6]\n",
        "    url_date = \"&date_debut_prelevement=\" + date_initial + \"&date_fin_prelevement=\" + date_final\n",
        "    url_end = \"&size=20000&sort=asc\"\n",
        "    \n",
        "    url = url_head + url_code + url_params + url_date + url_end\n",
        "\n",
        "    return url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df5JsSCtUO89"
      },
      "source": [
        "We can now call this function as follow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFmRJbGkUO89"
      },
      "source": [
        "url_ex = getHubeauURLquality(code_ex, date_initial_ex, date_final_ex, params_ex)\n",
        "\n",
        "print(url_ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdYgQqW4UO89"
      },
      "source": [
        "Now, we get and organize the content of this webpage into a pandas dataframe. Please note that a very large number of features are available. You can explore these features by visiting the [API website](https://hubeau.eaufrance.fr/) or by removing the last command in the following cell. \n",
        "\n",
        "Here we only keep following features:\n",
        "- the date of the measurement,\n",
        "- the code of the parameter measured,\n",
        "- the name of the parameter measured (in french),\n",
        "- the resulting value,\n",
        "- the quantification limit,\n",
        "- the detection limit,\n",
        "- the analytical uncertainty\n",
        "\n",
        "Finally we print the ten first lines of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQB9DhCuUO89"
      },
      "source": [
        "r = requests.get(url_ex)\n",
        "res = r.json()\n",
        "df = pd.DataFrame.from_dict(res[\"data\"])\n",
        "\n",
        "df = df.rename(columns={\"date_debut_prelevement\":\"date\",\n",
        "                        \"resultat\": \"value\"})\n",
        "\n",
        "df = df[['date', 'code_param', 'nom_param', 'value',\n",
        "         'limite_quantification','limite_detection', 'incertitude_analytique']]\n",
        "\n",
        "# We explicitly define the date in a datetime format\n",
        "df['date'] = pd.to_datetime(df['date'], format =  \"%Y-%m-%d\")\n",
        "df = df.set_index(\"date\")\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFhd54sGUO89"
      },
      "source": [
        "We can now plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcZ_4l7RUO89"
      },
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "ax = axs[0]\n",
        "ax.set_title('Temperature fluctuation at ' + mycode)\n",
        "\n",
        "# We plot the temperature fluctuation over the period\n",
        "df.loc[df.code_param == 1301]['value'].plot(ax = ax, \n",
        "              marker='o', \n",
        "              alpha=0.5)\n",
        "\n",
        "ax.set_ylabel('Temperature [°C]')\n",
        "ax.set_xlabel('')\n",
        "\n",
        "ax = axs[1]\n",
        "ax.set_title('Nitrates concentration fluctuation at ' + mycode)\n",
        "\n",
        "# We plot the nitrates fluctuation over the period\n",
        "df.loc[df.code_param == 1340]['value'].plot(ax = ax, \n",
        "              marker='o', \n",
        "              alpha=0.5)\n",
        "\n",
        "ax.set_ylabel('Concentration [mg/L]')\n",
        "ax.set_xlabel('')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isluixZdUO89"
      },
      "source": [
        "## Discussion\r\n",
        "\r\n",
        "This notebook intends to introduce some APIs and geo-webservices dedicated to the (hydro-)geological knowledge of France. This notebook has been reviewed and successfuly returns several parameter descriptions around an area of interest. Please note that all input data have been carefully checked, and be aware that an alteration of input data might raise errors and/or inconsistencies. Particularly, some site-specific choices have been made to illustrate different possibilities, and must not be generalized to other locations.\r\n",
        "\r\n",
        "Of course, readers are invited to play with all input parameters, and focus on other locations, but following advises and comments are given:\r\n",
        "- the area of interest should be carefully determined and must be of a reasonnable size to limit the execution time of different procedures. Particularly, lauching a data extraction of the all France is disadvised,\r\n",
        "- the closeness and depth of boreholes must be taken carefully when comparing some measurements: un-correlated fluctuations can be observed when sensors are note located into the same groundwater body or geological unit, even if boreholes are close.\r\n",
        "- there are many more available API functionnalities not discussed in this notebook. Please visit [Hubeau](https://hubeau.eaufrance.fr/) and [PIC'EAU](https://piceau.brgm.fr/) to find out more.\r\n",
        "- to work at reginal scales on several stations at a time, there are more efficient methods to extract and analyse data than ones presented in this notebook."
      ]
    }
  ]
}